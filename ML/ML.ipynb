{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Modelo de Machine Learning__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DATO IMPORTANTE!  Antes de correr cada modelo vuelva a correr el codigo para traer el dataframe DF de la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descargamos el mysql connector\n",
    "#pip install mysql-connector-python\n",
    "#pip install tensorflow.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import streamlit as st\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Conexion con la base de datos mysql en AWS RDS__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la conexión\n",
    "cnx = mysql.connector.connect(\n",
    "    host='db-latambrain-luis-mysql.ckwcpsorcjut.us-east-2.rds.amazonaws.com',\n",
    "    user='admin',\n",
    "    password='zGqS4YymBH9jsMOEFxoG',\n",
    "    database='sys'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matia\\AppData\\Local\\Temp\\ipykernel_10540\\3149902286.py:3: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, cnx)\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar una consulta SQL y guardar los resultados en un DataFrame de Pandas\n",
    "query = \"SELECT * FROM JobKaggle\"\n",
    "df = pd.read_sql(query, cnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work_year</th>\n",
       "      <th>experience_level</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>job_title</th>\n",
       "      <th>salary</th>\n",
       "      <th>salary_currency</th>\n",
       "      <th>salary_in_usd</th>\n",
       "      <th>employee_residence</th>\n",
       "      <th>remote_ratio</th>\n",
       "      <th>company_location</th>\n",
       "      <th>company_size</th>\n",
       "      <th>Column1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>MI</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>70000</td>\n",
       "      <td>EUR</td>\n",
       "      <td>79833</td>\n",
       "      <td>DE</td>\n",
       "      <td>0</td>\n",
       "      <td>DE</td>\n",
       "      <td>L</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Machine Learning Scientist</td>\n",
       "      <td>260000</td>\n",
       "      <td>USD</td>\n",
       "      <td>260000</td>\n",
       "      <td>JP</td>\n",
       "      <td>0</td>\n",
       "      <td>JP</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>85000</td>\n",
       "      <td>GBP</td>\n",
       "      <td>109024</td>\n",
       "      <td>GB</td>\n",
       "      <td>50</td>\n",
       "      <td>GB</td>\n",
       "      <td>M</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>MI</td>\n",
       "      <td>FT</td>\n",
       "      <td>Product Data Analyst</td>\n",
       "      <td>20000</td>\n",
       "      <td>USD</td>\n",
       "      <td>20000</td>\n",
       "      <td>HN</td>\n",
       "      <td>0</td>\n",
       "      <td>HN</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Machine Learning Engineer</td>\n",
       "      <td>150000</td>\n",
       "      <td>USD</td>\n",
       "      <td>150000</td>\n",
       "      <td>US</td>\n",
       "      <td>50</td>\n",
       "      <td>US</td>\n",
       "      <td>L</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>2022</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>154000</td>\n",
       "      <td>USD</td>\n",
       "      <td>154000</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "      <td>602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>2022</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>126000</td>\n",
       "      <td>USD</td>\n",
       "      <td>126000</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "      <td>603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>2022</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>129000</td>\n",
       "      <td>USD</td>\n",
       "      <td>129000</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "      <td>604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>2022</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>150000</td>\n",
       "      <td>USD</td>\n",
       "      <td>150000</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "      <td>605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>2022</td>\n",
       "      <td>MI</td>\n",
       "      <td>FT</td>\n",
       "      <td>AI Scientist</td>\n",
       "      <td>200000</td>\n",
       "      <td>USD</td>\n",
       "      <td>200000</td>\n",
       "      <td>IN</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>L</td>\n",
       "      <td>606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>607 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    work_year experience_level employment_type                   job_title  \\\n",
       "0        2020               MI              FT              Data Scientist   \n",
       "1        2020               SE              FT  Machine Learning Scientist   \n",
       "2        2020               SE              FT           Big Data Engineer   \n",
       "3        2020               MI              FT        Product Data Analyst   \n",
       "4        2020               SE              FT   Machine Learning Engineer   \n",
       "..        ...              ...             ...                         ...   \n",
       "602      2022               SE              FT               Data Engineer   \n",
       "603      2022               SE              FT               Data Engineer   \n",
       "604      2022               SE              FT                Data Analyst   \n",
       "605      2022               SE              FT                Data Analyst   \n",
       "606      2022               MI              FT                AI Scientist   \n",
       "\n",
       "     salary salary_currency salary_in_usd employee_residence remote_ratio  \\\n",
       "0     70000             EUR         79833                 DE            0   \n",
       "1    260000             USD        260000                 JP            0   \n",
       "2     85000             GBP        109024                 GB           50   \n",
       "3     20000             USD         20000                 HN            0   \n",
       "4    150000             USD        150000                 US           50   \n",
       "..      ...             ...           ...                ...          ...   \n",
       "602  154000             USD        154000                 US          100   \n",
       "603  126000             USD        126000                 US          100   \n",
       "604  129000             USD        129000                 US            0   \n",
       "605  150000             USD        150000                 US          100   \n",
       "606  200000             USD        200000                 IN          100   \n",
       "\n",
       "    company_location company_size  Column1  \n",
       "0                 DE            L        0  \n",
       "1                 JP            S        1  \n",
       "2                 GB            M        2  \n",
       "3                 HN            S        3  \n",
       "4                 US            L        4  \n",
       "..               ...          ...      ...  \n",
       "602               US            M      602  \n",
       "603               US            M      603  \n",
       "604               US            M      604  \n",
       "605               US            M      605  \n",
       "606               US            L      606  \n",
       "\n",
       "[607 rows x 12 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df #Revisamos la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work_year</th>\n",
       "      <th>experience_level</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>job_title</th>\n",
       "      <th>salary</th>\n",
       "      <th>salary_currency</th>\n",
       "      <th>salary_in_usd</th>\n",
       "      <th>employee_residence</th>\n",
       "      <th>remote_ratio</th>\n",
       "      <th>company_location</th>\n",
       "      <th>company_size</th>\n",
       "      <th>Column1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2021</td>\n",
       "      <td>MI</td>\n",
       "      <td>FT</td>\n",
       "      <td>BI Data Analyst</td>\n",
       "      <td>100000</td>\n",
       "      <td>USD</td>\n",
       "      <td>100000</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>2022</td>\n",
       "      <td>MI</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>170000</td>\n",
       "      <td>USD</td>\n",
       "      <td>170000</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>2021</td>\n",
       "      <td>MI</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>200000</td>\n",
       "      <td>USD</td>\n",
       "      <td>200000</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>L</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>2022</td>\n",
       "      <td>EX</td>\n",
       "      <td>FT</td>\n",
       "      <td>Analytics Engineer</td>\n",
       "      <td>135000</td>\n",
       "      <td>USD</td>\n",
       "      <td>135000</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>2022</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>116150</td>\n",
       "      <td>USD</td>\n",
       "      <td>116150</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "      <td>564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    work_year experience_level employment_type           job_title  salary  \\\n",
       "76       2021               MI              FT     BI Data Analyst  100000   \n",
       "294      2022               MI              FT       Data Engineer  170000   \n",
       "256      2021               MI              FT       Data Engineer  200000   \n",
       "368      2022               EX              FT  Analytics Engineer  135000   \n",
       "564      2022               SE              FT        Data Analyst  116150   \n",
       "\n",
       "    salary_currency salary_in_usd employee_residence remote_ratio  \\\n",
       "76              USD        100000                 US          100   \n",
       "294             USD        170000                 US          100   \n",
       "256             USD        200000                 US          100   \n",
       "368             USD        135000                 US          100   \n",
       "564             USD        116150                 US          100   \n",
       "\n",
       "    company_location company_size  Column1  \n",
       "76                US            M       76  \n",
       "294               US            M      294  \n",
       "256               US            L      256  \n",
       "368               US            M      368  \n",
       "564               US            M      564  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### _Este código utiliza el aprendizaje profundo (deep learning) para construir un modelo que pueda predecir el salario de un empleado en función de varias características, como su nivel de experiencia, tipo de empleo, título laboral, ubicación y tamaño de la empresa, entre otros. El modelo utilizado es una red neuronal artificial construida utilizando la librería Keras, que es una de las más populares para la construcción de modelos de aprendizaje profundo. El modelo es entrenado con datos de entrenamiento y validado con datos de prueba. Después de la formación, se evalúa el modelo utilizando la métrica de error cuadrático medio (MSE) y se realiza una predicción en un ejemplo de prueba._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Version 1 del modelo 1__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16/16 [==============================] - 1s 2ms/step - loss: 25655.0449\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 25398.7871\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 25108.9277\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 24747.7441\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 24298.1309\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 23722.4395\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 22967.3711\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 22006.8066\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 20795.2148\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 19326.1367\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 17622.4570\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 15702.6299\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 13557.0723\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 11435.6104\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 9445.4980\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7711.0200\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 6294.4761\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5377.9150\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4785.5684\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4498.7778\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4336.4170\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4209.8896\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4146.0908\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4083.7876\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4038.4473\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3995.4331\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3952.6648\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3922.3018\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3888.9224\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3863.6143\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3849.0266\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3815.9727\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3796.3167\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3780.2998\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3762.8433\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3744.9216\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3721.9382\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3712.3586\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3699.7271\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3682.2361\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3665.3445\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3660.8691\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3642.3579\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3621.6123\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3615.1277\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3599.1953\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3582.0024\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3576.5110\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3556.0696\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3556.0730\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3542.6584\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3523.7749\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3513.8472\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3501.2329\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3487.2009\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3478.4690\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3470.4622\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3459.3938\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3449.3740\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3441.0735\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3427.5325\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3422.5398\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3409.6079\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3394.9146\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3389.0061\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3379.1450\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3372.4763\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3353.8462\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3348.0181\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3331.4380\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3320.8899\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3318.8484\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3305.9021\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3294.1208\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3282.3530\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3273.2722\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3263.1929\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3256.8643\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3247.4719\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3250.2805\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3224.9656\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3209.3652\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3199.8936\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3188.4697\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3185.5681\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3174.5693\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3168.6472\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3153.9365\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3142.2336\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3137.0828\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3125.6699\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3114.6575\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3104.7759\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3093.1592\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3084.2349\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3072.6997\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3067.5520\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3059.0698\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3046.7446\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3037.6475\n",
      "Mean Squared Error: 4936.59814453125\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "Prediction: [[92.03922]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Codificación one-hot para las variables categóricas\n",
    "df_encoded = pd.get_dummies(df, columns=['experience_level', 'employment_type', 'job_title', 'salary_currency', 'employee_residence', 'company_location', 'company_size'])\n",
    "\n",
    "# Dividir el dataset en datos de entrenamiento y de prueba\n",
    "X = df_encoded.drop('salary', axis=1).values\n",
    "y = df_encoded['salary'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Escalar los datos para mejorar la eficiencia del modelo\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Definir el modelo\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compilamos y entrenamos el modelo\n",
    "model.compile(loss='mean_squared_error', optimizer=Adam()); model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluacion del modelo\n",
    "mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Hacemos una prediccion\n",
    "prediction = model.predict(X_test[:1])\n",
    "print(\"Prediction:\", prediction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Version 2 del modelo 1__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "8/8 [==============================] - 1s 3ms/step - loss: 25351.4062\n",
      "Epoch 2/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 24991.2949\n",
      "Epoch 3/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 24505.0762\n",
      "Epoch 4/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 23818.6445\n",
      "Epoch 5/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 22845.5273\n",
      "Epoch 6/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 21406.7832\n",
      "Epoch 7/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19381.8516\n",
      "Epoch 8/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 16656.2148\n",
      "Epoch 9/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13342.6904\n",
      "Epoch 10/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9508.3262\n",
      "Epoch 11/200\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6337.9712\n",
      "Epoch 12/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4748.8545\n",
      "Epoch 13/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4583.9150\n",
      "Epoch 14/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4344.2251\n",
      "Epoch 15/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4112.5620\n",
      "Epoch 16/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4018.0623\n",
      "Epoch 17/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3944.0598\n",
      "Epoch 18/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3872.8359\n",
      "Epoch 19/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3833.4473\n",
      "Epoch 20/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3785.2559\n",
      "Epoch 21/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3756.6414\n",
      "Epoch 22/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3715.6191\n",
      "Epoch 23/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3693.1409\n",
      "Epoch 24/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3665.4307\n",
      "Epoch 25/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3637.7864\n",
      "Epoch 26/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3612.4429\n",
      "Epoch 27/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3589.8745\n",
      "Epoch 28/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3576.2793\n",
      "Epoch 29/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3549.7214\n",
      "Epoch 30/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3516.4219\n",
      "Epoch 31/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3503.4221\n",
      "Epoch 32/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3486.3269\n",
      "Epoch 33/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3463.1687\n",
      "Epoch 34/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3447.6489\n",
      "Epoch 35/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3429.3638\n",
      "Epoch 36/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3415.8228\n",
      "Epoch 37/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3396.6702\n",
      "Epoch 38/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3375.9705\n",
      "Epoch 39/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3360.6289\n",
      "Epoch 40/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3346.2678\n",
      "Epoch 41/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3329.3059\n",
      "Epoch 42/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3316.6580\n",
      "Epoch 43/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3299.1870\n",
      "Epoch 44/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3285.3926\n",
      "Epoch 45/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3266.6323\n",
      "Epoch 46/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3250.0549\n",
      "Epoch 47/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3240.1055\n",
      "Epoch 48/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3223.2273\n",
      "Epoch 49/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3210.2808\n",
      "Epoch 50/200\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3191.7910\n",
      "Epoch 51/200\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3178.8345\n",
      "Epoch 52/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3157.7764\n",
      "Epoch 53/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3142.4644\n",
      "Epoch 54/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3129.5994\n",
      "Epoch 55/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 3112.2454\n",
      "Epoch 56/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3098.9268\n",
      "Epoch 57/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3086.9443\n",
      "Epoch 58/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3067.9187\n",
      "Epoch 59/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3050.1729\n",
      "Epoch 60/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3033.3313\n",
      "Epoch 61/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3020.1418\n",
      "Epoch 62/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3002.4407\n",
      "Epoch 63/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2989.6062\n",
      "Epoch 64/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2976.3989\n",
      "Epoch 65/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2959.0474\n",
      "Epoch 66/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2939.8372\n",
      "Epoch 67/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2923.6589\n",
      "Epoch 68/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2910.5359\n",
      "Epoch 69/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2903.7524\n",
      "Epoch 70/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2877.7649\n",
      "Epoch 71/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2861.3499\n",
      "Epoch 72/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2842.1855\n",
      "Epoch 73/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2828.1040\n",
      "Epoch 74/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2804.7415\n",
      "Epoch 75/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2786.3230\n",
      "Epoch 76/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2769.6235\n",
      "Epoch 77/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2754.8008\n",
      "Epoch 78/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2732.0000\n",
      "Epoch 79/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2716.6917\n",
      "Epoch 80/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2697.6199\n",
      "Epoch 81/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2677.4302\n",
      "Epoch 82/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2655.3794\n",
      "Epoch 83/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2633.8884\n",
      "Epoch 84/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2619.7266\n",
      "Epoch 85/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2596.5894\n",
      "Epoch 86/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2577.8877\n",
      "Epoch 87/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2558.1353\n",
      "Epoch 88/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2534.8965\n",
      "Epoch 89/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2514.7358\n",
      "Epoch 90/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2496.1047\n",
      "Epoch 91/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 2476.0969\n",
      "Epoch 92/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2457.2964\n",
      "Epoch 93/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2447.8374\n",
      "Epoch 94/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2413.5410\n",
      "Epoch 95/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2396.9805\n",
      "Epoch 96/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2371.1592\n",
      "Epoch 97/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2353.1770\n",
      "Epoch 98/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2335.1787\n",
      "Epoch 99/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2316.0422\n",
      "Epoch 100/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2297.5190\n",
      "Epoch 101/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2289.6187\n",
      "Epoch 102/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2240.8752\n",
      "Epoch 103/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2235.3513\n",
      "Epoch 104/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2205.8154\n",
      "Epoch 105/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2183.8105\n",
      "Epoch 106/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2169.1672\n",
      "Epoch 107/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2131.1609\n",
      "Epoch 108/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2110.8250\n",
      "Epoch 109/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2085.0227\n",
      "Epoch 110/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2067.4343\n",
      "Epoch 111/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2045.6782\n",
      "Epoch 112/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 2023.3680\n",
      "Epoch 113/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1996.7881\n",
      "Epoch 114/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1976.5852\n",
      "Epoch 115/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1954.5547\n",
      "Epoch 116/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1937.0819\n",
      "Epoch 117/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1910.8864\n",
      "Epoch 118/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1895.0305\n",
      "Epoch 119/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1885.1147\n",
      "Epoch 120/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1856.5547\n",
      "Epoch 121/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1848.8486\n",
      "Epoch 122/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1832.2832\n",
      "Epoch 123/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1802.5540\n",
      "Epoch 124/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1778.4674\n",
      "Epoch 125/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1759.3209\n",
      "Epoch 126/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1734.7240\n",
      "Epoch 127/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1716.7505\n",
      "Epoch 128/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1714.9126\n",
      "Epoch 129/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1697.0149\n",
      "Epoch 130/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1656.4651\n",
      "Epoch 131/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1647.5540\n",
      "Epoch 132/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1630.8926\n",
      "Epoch 133/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1609.4403\n",
      "Epoch 134/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1598.2091\n",
      "Epoch 135/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1569.3374\n",
      "Epoch 136/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1546.5210\n",
      "Epoch 137/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1535.9849\n",
      "Epoch 138/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1533.4749\n",
      "Epoch 139/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1512.1554\n",
      "Epoch 140/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1489.8319\n",
      "Epoch 141/200\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1469.1366\n",
      "Epoch 142/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1454.6083\n",
      "Epoch 143/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1431.5365\n",
      "Epoch 144/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1424.5138\n",
      "Epoch 145/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1395.1575\n",
      "Epoch 146/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1393.8434\n",
      "Epoch 147/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1364.4868\n",
      "Epoch 148/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1344.9910\n",
      "Epoch 149/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1324.2041\n",
      "Epoch 150/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1315.1178\n",
      "Epoch 151/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1300.3209\n",
      "Epoch 152/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1289.9425\n",
      "Epoch 153/200\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1256.1968\n",
      "Epoch 154/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1264.5988\n",
      "Epoch 155/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1232.7723\n",
      "Epoch 156/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1230.5442\n",
      "Epoch 157/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1207.1136\n",
      "Epoch 158/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1192.7020\n",
      "Epoch 159/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1173.6302\n",
      "Epoch 160/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1170.7313\n",
      "Epoch 161/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1206.8690\n",
      "Epoch 162/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1179.3876\n",
      "Epoch 163/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1145.1948\n",
      "Epoch 164/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1141.7660\n",
      "Epoch 165/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1103.3937\n",
      "Epoch 166/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1107.9766\n",
      "Epoch 167/200\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1091.2418\n",
      "Epoch 168/200\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1072.6177\n",
      "Epoch 169/200\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1077.0178\n",
      "Epoch 170/200\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1055.6251\n",
      "Epoch 171/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1044.1353\n",
      "Epoch 172/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1017.7441\n",
      "Epoch 173/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1016.0544\n",
      "Epoch 174/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 987.5115\n",
      "Epoch 175/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 979.3231\n",
      "Epoch 176/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 981.1009\n",
      "Epoch 177/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 987.9595\n",
      "Epoch 178/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 960.0604\n",
      "Epoch 179/200\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 946.1414\n",
      "Epoch 180/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 932.2059\n",
      "Epoch 181/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 934.4009\n",
      "Epoch 182/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 933.3754\n",
      "Epoch 183/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 908.3010\n",
      "Epoch 184/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 875.6907\n",
      "Epoch 185/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 862.2441\n",
      "Epoch 186/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 854.4479\n",
      "Epoch 187/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 850.5262\n",
      "Epoch 188/200\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 836.5732\n",
      "Epoch 189/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 821.0062\n",
      "Epoch 190/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 848.6139\n",
      "Epoch 191/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 810.1482\n",
      "Epoch 192/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 819.8986\n",
      "Epoch 193/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 800.9354\n",
      "Epoch 194/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 778.2732\n",
      "Epoch 195/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 775.4073\n",
      "Epoch 196/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 768.6049\n",
      "Epoch 197/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 748.1251\n",
      "Epoch 198/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 732.6747\n",
      "Epoch 199/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 723.6118\n",
      "Epoch 200/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 730.4568\n",
      "Mean Squared Error: 7126.9365234375\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "Prediction: [[61.523914]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Codificación one-hot para las variables categóricas\n",
    "df_encoded = pd.get_dummies(df, columns=['experience_level', 'employment_type', 'job_title', 'salary_currency', 'employee_residence', 'company_location', 'company_size'])\n",
    "\n",
    "# Dividir el dataset en datos de entrenamiento y de prueba\n",
    "X = df_encoded.drop('salary', axis=1).values\n",
    "y = df_encoded['salary'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Escalar los datos para mejorar la eficiencia del modelo\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Definir el modelo\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(loss='mean_squared_error', optimizer=Adam())\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=64, verbose=1)\n",
    "\n",
    "# Evaluar el modelo\n",
    "mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Realizar una predicción\n",
    "prediction1 = model.predict(X_test[:1])\n",
    "print(\"Prediction:\", prediction1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Version 3 del modelo 1__ [SELECCIONADO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "2023-04-20 22:22:25.343 WARNING absl: `lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "7/7 [==============================] - 2s 35ms/step - loss: 25477.0312 - val_loss: 27054.4355\n",
      "Epoch 2/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 25381.9219 - val_loss: 26998.4375\n",
      "Epoch 3/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 25340.7148 - val_loss: 26967.4707\n",
      "Epoch 4/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 25319.0410 - val_loss: 26934.4766\n",
      "Epoch 5/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 25276.4746 - val_loss: 26889.3809\n",
      "Epoch 6/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 25225.0137 - val_loss: 26823.0156\n",
      "Epoch 7/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 25150.1309 - val_loss: 26730.8809\n",
      "Epoch 8/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 25043.9707 - val_loss: 26600.0781\n",
      "Epoch 9/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 24897.7168 - val_loss: 26418.1680\n",
      "Epoch 10/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 24646.4922 - val_loss: 26156.9336\n",
      "Epoch 11/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 24264.4121 - val_loss: 25787.5000\n",
      "Epoch 12/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 23738.4102 - val_loss: 25255.2168\n",
      "Epoch 13/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 23046.4121 - val_loss: 24458.2910\n",
      "Epoch 14/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 21913.6309 - val_loss: 23304.8691\n",
      "Epoch 15/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 20345.3574 - val_loss: 21735.9023\n",
      "Epoch 16/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 18272.3633 - val_loss: 19718.9277\n",
      "Epoch 17/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 15704.8750 - val_loss: 17313.2598\n",
      "Epoch 18/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 12641.7930 - val_loss: 14569.5117\n",
      "Epoch 19/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 10007.4404 - val_loss: 11905.1807\n",
      "Epoch 20/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 7644.0444 - val_loss: 9913.9355\n",
      "Epoch 21/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 6299.0874 - val_loss: 8897.0918\n",
      "Epoch 22/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 6195.0786 - val_loss: 8488.6504\n",
      "Epoch 23/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 5628.7695 - val_loss: 8415.0010\n",
      "Epoch 24/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 6215.6841 - val_loss: 8610.9668\n",
      "Epoch 25/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 6211.3643 - val_loss: 8736.0938\n",
      "Epoch 26/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 5833.7720 - val_loss: 8478.8262\n",
      "Epoch 27/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 5441.5317 - val_loss: 8176.1226\n",
      "Epoch 28/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 5461.1489 - val_loss: 8121.1953\n",
      "Epoch 29/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 5519.1992 - val_loss: 8195.7451\n",
      "Epoch 30/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 5677.5220 - val_loss: 8269.7783\n",
      "Epoch 31/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 5264.4966 - val_loss: 8275.7852\n",
      "Epoch 32/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 5830.7886 - val_loss: 8279.8213\n",
      "Epoch 33/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 4653.6777 - val_loss: 8198.3574\n",
      "Epoch 34/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 5356.0298 - val_loss: 8186.6919\n",
      "Epoch 35/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 5090.1162 - val_loss: 8148.6367\n",
      "Epoch 36/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 5397.7124 - val_loss: 8175.6084\n",
      "Epoch 37/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 5003.7798 - val_loss: 8265.0010\n",
      "Epoch 38/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 4772.4468 - val_loss: 8290.8252\n",
      "Epoch 39/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 5070.5181 - val_loss: 8329.0254\n",
      "Epoch 40/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 4919.5195 - val_loss: 8298.6855\n",
      "Epoch 41/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 5170.9956 - val_loss: 8230.1748\n",
      "Epoch 42/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 5151.3970 - val_loss: 8190.4087\n",
      "Epoch 43/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4797.4980 - val_loss: 8197.3428\n",
      "Epoch 44/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 4857.7212 - val_loss: 8202.9971\n",
      "Epoch 45/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4720.3286 - val_loss: 8206.0107\n",
      "Epoch 46/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 4924.5820 - val_loss: 8210.5850\n",
      "Epoch 47/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 5000.8457 - val_loss: 8184.0225\n",
      "Epoch 48/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4929.0864 - val_loss: 8154.4380\n",
      "Epoch 49/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4892.3511 - val_loss: 8055.6899\n",
      "Epoch 50/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 4757.9766 - val_loss: 7993.2617\n",
      "Epoch 51/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 5038.6826 - val_loss: 7980.2339\n",
      "Epoch 52/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 4825.7671 - val_loss: 7991.5728\n",
      "Epoch 53/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 4819.0029 - val_loss: 7994.9258\n",
      "Epoch 54/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4876.9092 - val_loss: 7951.0117\n",
      "Epoch 55/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4882.9053 - val_loss: 7931.1538\n",
      "Epoch 56/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4952.4087 - val_loss: 7960.5166\n",
      "Epoch 57/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 5256.6201 - val_loss: 7936.1235\n",
      "Epoch 58/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4928.1196 - val_loss: 7879.1699\n",
      "Epoch 59/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4657.1812 - val_loss: 7909.3828\n",
      "Epoch 60/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 4535.8486 - val_loss: 7912.0269\n",
      "Epoch 61/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4830.7012 - val_loss: 7851.4976\n",
      "Epoch 62/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4815.0981 - val_loss: 7821.3472\n",
      "Epoch 63/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 4817.0840 - val_loss: 7822.1274\n",
      "Epoch 64/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 4634.5039 - val_loss: 7860.4531\n",
      "Epoch 65/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 4825.8062 - val_loss: 7826.3423\n",
      "Epoch 66/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 4923.9614 - val_loss: 7706.1113\n",
      "Epoch 67/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 4657.2080 - val_loss: 7639.1406\n",
      "Epoch 68/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 4879.1519 - val_loss: 7601.8027\n",
      "Epoch 69/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 4593.9478 - val_loss: 7568.4854\n",
      "Epoch 70/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 4686.0142 - val_loss: 7546.5747\n",
      "Epoch 71/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4467.8276 - val_loss: 7618.7358\n",
      "Epoch 72/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 4812.8076 - val_loss: 7705.8838\n",
      "Epoch 73/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 4610.5356 - val_loss: 7762.7446\n",
      "Epoch 74/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4781.4922 - val_loss: 7806.6191\n",
      "Epoch 75/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 4518.5703 - val_loss: 7747.0586\n",
      "Epoch 76/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4369.6509 - val_loss: 7706.3691\n",
      "Epoch 77/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 4487.5601 - val_loss: 7652.6328\n",
      "Epoch 78/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4649.8145 - val_loss: 7625.8501\n",
      "Epoch 79/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4171.2124 - val_loss: 7649.2070\n",
      "Epoch 80/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4532.9370 - val_loss: 7691.3594\n",
      "Epoch 81/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4763.7261 - val_loss: 7654.7700\n",
      "Epoch 82/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4467.9653 - val_loss: 7589.5547\n",
      "Epoch 83/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4530.7876 - val_loss: 7516.3574\n",
      "Epoch 84/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4776.4668 - val_loss: 7463.8145\n",
      "Epoch 85/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4340.5601 - val_loss: 7472.5054\n",
      "Epoch 86/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4396.9609 - val_loss: 7444.2822\n",
      "Epoch 87/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4453.3252 - val_loss: 7421.1919\n",
      "Epoch 88/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4513.3042 - val_loss: 7374.4336\n",
      "Epoch 89/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 4709.6382 - val_loss: 7377.7676\n",
      "Epoch 90/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4972.2373 - val_loss: 7399.0684\n",
      "Epoch 91/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 3897.6917 - val_loss: 7397.3857\n",
      "Epoch 92/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 4643.2295 - val_loss: 7381.9834\n",
      "Epoch 93/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 4505.6519 - val_loss: 7340.8442\n",
      "Epoch 94/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 4552.3872 - val_loss: 7311.4795\n",
      "Epoch 95/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 4681.0459 - val_loss: 7265.1729\n",
      "Epoch 96/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 4054.0073 - val_loss: 7201.3232\n",
      "Epoch 97/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 4292.7642 - val_loss: 7146.4268\n",
      "Epoch 98/200\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 4554.5308 - val_loss: 7124.0283\n",
      "Epoch 99/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4673.3179 - val_loss: 7117.9536\n",
      "Epoch 100/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4338.3647 - val_loss: 7081.3828\n",
      "Epoch 101/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4137.9150 - val_loss: 7070.3545\n",
      "Epoch 102/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 3887.4426 - val_loss: 7100.2578\n",
      "Epoch 103/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4478.7905 - val_loss: 7112.6255\n",
      "Epoch 104/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4164.2393 - val_loss: 7058.9067\n",
      "Epoch 105/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 4262.7080 - val_loss: 7031.3584\n",
      "Epoch 106/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 4632.8091 - val_loss: 7012.4019\n",
      "Epoch 107/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 4293.9229 - val_loss: 6992.0391\n",
      "Epoch 108/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4026.2456 - val_loss: 6953.5137\n",
      "Epoch 109/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 4149.0991 - val_loss: 6921.2705\n",
      "Epoch 110/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4069.4998 - val_loss: 6926.6855\n",
      "Epoch 111/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 4304.5630 - val_loss: 6893.1455\n",
      "Epoch 112/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4556.5845 - val_loss: 6845.8555\n",
      "Epoch 113/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4291.4116 - val_loss: 6864.5317\n",
      "Epoch 114/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4031.6199 - val_loss: 6897.4795\n",
      "Epoch 115/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 4314.1113 - val_loss: 6932.3159\n",
      "Epoch 116/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 4083.4883 - val_loss: 6911.3667\n",
      "Epoch 117/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 4340.7495 - val_loss: 6813.5498\n",
      "Epoch 118/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4252.2568 - val_loss: 6751.0405\n",
      "Epoch 119/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 3880.5850 - val_loss: 6700.8506\n",
      "Epoch 120/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4140.9390 - val_loss: 6701.0962\n",
      "Epoch 121/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4319.5103 - val_loss: 6720.3950\n",
      "Epoch 122/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 4121.5718 - val_loss: 6777.6528\n",
      "Epoch 123/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3885.9023 - val_loss: 6797.8369\n",
      "Epoch 124/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4241.4302 - val_loss: 6860.6831\n",
      "Epoch 125/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4206.9824 - val_loss: 6835.6191\n",
      "Epoch 126/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 4275.3560 - val_loss: 6773.1211\n",
      "Epoch 127/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4034.8184 - val_loss: 6753.0239\n",
      "Epoch 128/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 3955.5200 - val_loss: 6754.8877\n",
      "Epoch 129/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 4055.0981 - val_loss: 6790.9194\n",
      "Epoch 130/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 4111.1909 - val_loss: 6739.2495\n",
      "Epoch 131/200\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 3883.0210 - val_loss: 6684.8647\n",
      "Epoch 132/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 4154.6938 - val_loss: 6664.8584\n",
      "Epoch 133/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 4111.1255 - val_loss: 6600.1274\n",
      "Epoch 134/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3816.8748 - val_loss: 6519.8203\n",
      "Epoch 135/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 4195.4375 - val_loss: 6494.5298\n",
      "Epoch 136/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3560.5251 - val_loss: 6446.5278\n",
      "Epoch 137/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3722.6399 - val_loss: 6454.9419\n",
      "Epoch 138/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3843.7341 - val_loss: 6467.6919\n",
      "Epoch 139/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 3654.9648 - val_loss: 6483.1470\n",
      "Epoch 140/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 3763.4492 - val_loss: 6447.4839\n",
      "Epoch 141/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 4052.5261 - val_loss: 6463.8945\n",
      "Epoch 142/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4098.5264 - val_loss: 6488.6880\n",
      "Epoch 143/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4083.3560 - val_loss: 6448.2627\n",
      "Epoch 144/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 4313.7778 - val_loss: 6398.7524\n",
      "Epoch 145/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3936.6389 - val_loss: 6316.5117\n",
      "Epoch 146/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4065.6111 - val_loss: 6250.8911\n",
      "Epoch 147/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3937.1953 - val_loss: 6277.5039\n",
      "Epoch 148/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4199.9360 - val_loss: 6275.9292\n",
      "Epoch 149/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3908.9304 - val_loss: 6243.4458\n",
      "Epoch 150/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3878.8582 - val_loss: 6191.0015\n",
      "Epoch 151/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 3994.5371 - val_loss: 6170.2588\n",
      "Epoch 152/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3755.9741 - val_loss: 6150.5356\n",
      "Epoch 153/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 3949.5627 - val_loss: 6114.1025\n",
      "Epoch 154/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3704.6421 - val_loss: 6146.1172\n",
      "Epoch 155/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 4200.7627 - val_loss: 6149.9614\n",
      "Epoch 156/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 4145.6519 - val_loss: 6119.3208\n",
      "Epoch 157/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 3762.7610 - val_loss: 6092.1660\n",
      "Epoch 158/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 3535.1831 - val_loss: 6095.2202\n",
      "Epoch 159/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3880.9810 - val_loss: 6070.9639\n",
      "Epoch 160/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3994.4963 - val_loss: 6058.4731\n",
      "Epoch 161/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 3991.0542 - val_loss: 6024.9644\n",
      "Epoch 162/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 3895.7832 - val_loss: 6012.9492\n",
      "Epoch 163/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3818.0945 - val_loss: 5973.3623\n",
      "Epoch 164/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 3530.3970 - val_loss: 5986.8857\n",
      "Epoch 165/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 3851.6611 - val_loss: 5982.5518\n",
      "Epoch 166/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 3972.1533 - val_loss: 5948.4219\n",
      "Epoch 167/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 3613.4741 - val_loss: 5912.0601\n",
      "Epoch 168/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3770.6594 - val_loss: 5866.4800\n",
      "Epoch 169/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3727.9539 - val_loss: 5882.5762\n",
      "Epoch 170/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3870.6865 - val_loss: 5931.5376\n",
      "Epoch 171/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3686.4336 - val_loss: 5975.5308\n",
      "Epoch 172/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3888.2285 - val_loss: 6045.1416\n",
      "Epoch 173/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3736.2217 - val_loss: 5967.5078\n",
      "Epoch 174/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4142.4463 - val_loss: 5851.0391\n",
      "Epoch 175/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3984.9648 - val_loss: 5781.5234\n",
      "Epoch 176/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3485.8647 - val_loss: 5771.9355\n",
      "Epoch 177/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3700.0132 - val_loss: 5803.3140\n",
      "Epoch 178/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3591.0161 - val_loss: 5818.1714\n",
      "Epoch 179/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3951.8267 - val_loss: 5861.5308\n",
      "Epoch 180/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3352.1340 - val_loss: 5833.2153\n",
      "Epoch 181/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3795.5286 - val_loss: 5823.2036\n",
      "Epoch 182/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3724.5071 - val_loss: 5829.1733\n",
      "Epoch 183/200\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 3574.6191 - val_loss: 5844.8301\n",
      "Epoch 184/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4037.5503 - val_loss: 5859.3071\n",
      "Epoch 185/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 3962.8218 - val_loss: 5858.2417\n",
      "Epoch 186/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 3875.4258 - val_loss: 5859.0215\n",
      "Epoch 187/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 3860.7571 - val_loss: 5855.5742\n",
      "Epoch 188/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 3727.1716 - val_loss: 5874.2798\n",
      "Epoch 189/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3755.8691 - val_loss: 5949.0405\n",
      "Epoch 190/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 3874.0776 - val_loss: 5904.7134\n",
      "Epoch 191/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3689.8884 - val_loss: 5861.0601\n",
      "Epoch 192/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3361.7866 - val_loss: 5880.5806\n",
      "Epoch 193/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 3516.3174 - val_loss: 5916.7021\n",
      "Epoch 194/200\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 3625.7932 - val_loss: 5856.5000\n",
      "Epoch 195/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3745.0996 - val_loss: 5831.9614\n",
      "Epoch 196/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3540.1323 - val_loss: 5886.5039\n",
      "Epoch 197/200\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3577.7866 - val_loss: 5957.5850\n",
      "Epoch 198/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3832.6208 - val_loss: 5943.3662\n",
      "Epoch 199/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3277.5396 - val_loss: 5900.1660\n",
      "Epoch 200/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3779.7659 - val_loss: 5919.6016\n",
      "Mean Squared Error: 4711.6220703125\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "Prediction2: [[87.80899]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Codificación one-hot para las variables categóricas\n",
    "df_encoded = pd.get_dummies(df, columns=['experience_level', 'employment_type', 'job_title', 'salary_currency', 'employee_residence', 'company_location', 'company_size'])\n",
    "\n",
    "# Dividir el dataset en datos de entrenamiento y de prueba\n",
    "X = df_encoded.drop('salary', axis=1).values\n",
    "y = df_encoded['salary'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Escalar los datos para mejorar la eficiencia del modelo\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Definir el modelo\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(8, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.001))\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=64, verbose=1, validation_split=0.2)\n",
    "\n",
    "# Evaluar el modelo\n",
    "mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "# Realizar una predicción\n",
    "prediction2 = model.predict(X_test[:1])\n",
    "print(\"Prediction2:\", prediction)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Version 4 del modelo 1__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7/7 [==============================] - 2s 40ms/step - loss: 25201.9648 - val_loss: 26538.8965\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 24791.5820 - val_loss: 26016.9609\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 24242.2852 - val_loss: 25260.4121\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 23384.4941 - val_loss: 24164.6719\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 22049.9023 - val_loss: 22581.8652\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 20136.1230 - val_loss: 20390.6875\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 17404.0410 - val_loss: 17546.2109\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 13987.5635 - val_loss: 14217.1045\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 10221.1084 - val_loss: 10846.9033\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 6814.6172 - val_loss: 8362.0615\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 5092.7788 - val_loss: 7443.9478\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 4866.7671 - val_loss: 7393.7402\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 4769.7637 - val_loss: 7385.0220\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 4418.7617 - val_loss: 7368.0659\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 4222.2505 - val_loss: 7446.7969\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 4122.3213 - val_loss: 7474.1089\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4055.3047 - val_loss: 7442.3584\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 4013.1270 - val_loss: 7325.9922\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 3968.2825 - val_loss: 7264.6016\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3917.2483 - val_loss: 7299.7158\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3886.0088 - val_loss: 7260.1660\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3839.8538 - val_loss: 7175.9399\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3806.3254 - val_loss: 7108.4038\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3777.8867 - val_loss: 7077.7759\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3725.0693 - val_loss: 7103.2334\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 3720.9192 - val_loss: 7114.6714\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 3697.0491 - val_loss: 7130.3853\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 3668.2744 - val_loss: 7099.5264\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 3641.0400 - val_loss: 7102.5425\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3660.9941 - val_loss: 7138.8687\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3680.7571 - val_loss: 7187.6929\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3663.0244 - val_loss: 7146.4302\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3587.7119 - val_loss: 7056.4375\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3552.6963 - val_loss: 7003.5278\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3528.6228 - val_loss: 7010.2305\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3562.8335 - val_loss: 7052.3442\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3520.0825 - val_loss: 6894.8892\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3487.0571 - val_loss: 6838.8789\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3498.9331 - val_loss: 6757.4058\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3468.7437 - val_loss: 6755.2539\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3443.4475 - val_loss: 6767.8081\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 3430.0886 - val_loss: 6791.0508\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3413.7832 - val_loss: 6763.4409\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3405.1299 - val_loss: 6773.2646\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 3396.3879 - val_loss: 6709.9111\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3375.7600 - val_loss: 6588.9741\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 3397.5129 - val_loss: 6500.5605\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3377.7371 - val_loss: 6481.1167\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3349.6838 - val_loss: 6509.0933\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 3334.0461 - val_loss: 6586.7466\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 3318.9536 - val_loss: 6561.1929\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3304.6191 - val_loss: 6472.2495\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3301.1936 - val_loss: 6376.8052\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3301.1863 - val_loss: 6352.5771\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3280.5798 - val_loss: 6367.7295\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3282.8601 - val_loss: 6420.6997\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3270.3777 - val_loss: 6361.0220\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3253.0205 - val_loss: 6321.9639\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 3230.3127 - val_loss: 6310.2075\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 3215.8689 - val_loss: 6293.0508\n",
      "Epoch 61/100\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 3199.4277 - val_loss: 6285.3164\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 3192.2087 - val_loss: 6294.4399\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3203.6843 - val_loss: 6418.3613\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3268.2729 - val_loss: 6489.1304\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3224.7683 - val_loss: 6357.8081\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3174.2900 - val_loss: 6296.1211\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3151.6360 - val_loss: 6228.6030\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3122.4033 - val_loss: 6220.2798\n",
      "Epoch 69/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3109.1716 - val_loss: 6171.0786\n",
      "Epoch 70/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3092.2583 - val_loss: 6133.9150\n",
      "Epoch 71/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 3080.2622 - val_loss: 6179.0298\n",
      "Epoch 72/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 3071.4185 - val_loss: 6183.1934\n",
      "Epoch 73/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3083.4587 - val_loss: 6082.7559\n",
      "Epoch 74/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 3068.6262 - val_loss: 6089.1777\n",
      "Epoch 75/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 3067.6936 - val_loss: 5995.9653\n",
      "Epoch 76/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3057.3374 - val_loss: 5998.4390\n",
      "Epoch 77/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 3048.4414 - val_loss: 6030.5786\n",
      "Epoch 78/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 3011.9517 - val_loss: 6004.9111\n",
      "Epoch 79/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 2992.9282 - val_loss: 5980.2822\n",
      "Epoch 80/100\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 2960.9324 - val_loss: 5953.1738\n",
      "Epoch 81/100\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 2980.6223 - val_loss: 5940.7920\n",
      "Epoch 82/100\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 2948.5684 - val_loss: 5888.9653\n",
      "Epoch 83/100\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 2931.0718 - val_loss: 5978.9937\n",
      "Epoch 84/100\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 2956.8669 - val_loss: 6230.4521\n",
      "Epoch 85/100\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 3039.6743 - val_loss: 6183.9805\n",
      "Epoch 86/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 2947.8308 - val_loss: 6112.1001\n",
      "Epoch 87/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 2911.8843 - val_loss: 6155.9331\n",
      "Epoch 88/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 2897.8801 - val_loss: 6128.3120\n",
      "Epoch 89/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 2872.4404 - val_loss: 5918.1274\n",
      "Epoch 90/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 2841.6824 - val_loss: 5842.0688\n",
      "Epoch 91/100\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 2820.9690 - val_loss: 5894.8491\n",
      "Epoch 92/100\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 2810.5354 - val_loss: 5966.6802\n",
      "Epoch 93/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 2792.1956 - val_loss: 5906.5024\n",
      "Epoch 94/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 2771.2104 - val_loss: 5888.7866\n",
      "Epoch 95/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 2773.1646 - val_loss: 5896.1113\n",
      "Epoch 96/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 2731.2502 - val_loss: 6001.0181\n",
      "Epoch 97/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 2721.7913 - val_loss: 5994.9717\n",
      "Epoch 98/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 2709.1675 - val_loss: 5996.3633\n",
      "Epoch 99/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 2679.1841 - val_loss: 6103.4111\n",
      "Epoch 100/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 2662.3511 - val_loss: 6229.4229\n",
      "Epoch 100: early stopping\n",
      "Mean Squared Error: 5446.732421875\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "Prediction: [[87.73058]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Codificación one-hot para las variables categóricas\n",
    "cat_vars = ['experience_level', 'employment_type', 'job_title', 'salary_currency', 'employee_residence', 'company_location', 'company_size']\n",
    "df_encoded = pd.get_dummies(df, columns=cat_vars)\n",
    "\n",
    "# Dividir el dataset en datos de entrenamiento y de prueba\n",
    "X = df_encoded.drop('salary', axis=1).values\n",
    "y = df_encoded['salary'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Escalar los datos para mejorar la eficiencia del modelo\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Definir el modelo\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(loss='mean_squared_error', optimizer=Adam())\n",
    "\n",
    "# Entrenar el modelo\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=64, verbose=1, validation_split=0.2, callbacks=[early_stop])\n",
    "\n",
    "# Evaluar el modelo\n",
    "mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Realizar una predicción\n",
    "prediction = model.predict(X_test[:1])\n",
    "print(\"Prediction:\", prediction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ___MODELO 1 QUE DICE SI EL PRECIO ES ALTO O BAJO___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "2023-04-20 23:08:30.733 `lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 35796.6094 - val_loss: 39860.8242\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 35771.9688 - val_loss: 39858.8594\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 35777.4961 - val_loss: 39856.8164\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 35777.9258 - val_loss: 39854.6055\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 35764.0273 - val_loss: 39852.4648\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 35739.5234 - val_loss: 39850.3438\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 35736.8633 - val_loss: 39846.7539\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 35709.7891 - val_loss: 39842.2461\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 35738.3516 - val_loss: 39837.1523\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 35677.6172 - val_loss: 39830.8516\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 35699.7734 - val_loss: 39822.6172\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 35721.3867 - val_loss: 39813.1289\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 35685.0508 - val_loss: 39802.0664\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 35626.0156 - val_loss: 39788.1406\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 35634.7383 - val_loss: 39772.8125\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 35555.6367 - val_loss: 39756.7930\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 35586.4961 - val_loss: 39739.6055\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 35621.6875 - val_loss: 39721.5195\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 35551.5742 - val_loss: 39702.1289\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 35440.4453 - val_loss: 39681.6367\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 35485.0469 - val_loss: 39660.4062\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 35358.5078 - val_loss: 39638.0352\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 35391.0273 - val_loss: 39615.5039\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 35456.5664 - val_loss: 39592.8164\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 35411.9062 - val_loss: 39569.0664\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 35293.8477 - val_loss: 39543.6133\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 35425.1992 - val_loss: 39517.5586\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 35334.5898 - val_loss: 39490.5312\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 35294.9961 - val_loss: 39462.7617\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 35120.9180 - val_loss: 39431.7695\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 35077.6758 - val_loss: 39398.6406\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 35072.0859 - val_loss: 39364.0273\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 34877.5859 - val_loss: 39326.5703\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 35000.0117 - val_loss: 39286.7305\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 34867.8945 - val_loss: 39243.3516\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 34621.9336 - val_loss: 39197.5508\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 34733.9180 - val_loss: 39149.7031\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 34839.9102 - val_loss: 39100.1289\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 34471.1094 - val_loss: 39047.5352\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 34301.4805 - val_loss: 38992.5742\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 34302.2578 - val_loss: 38935.4766\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 34404.5156 - val_loss: 38877.2969\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 34037.1289 - val_loss: 38816.1875\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 34352.2812 - val_loss: 38752.5742\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 34236.8438 - val_loss: 38686.4375\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 33933.9180 - val_loss: 38617.6250\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 33854.2148 - val_loss: 38544.8359\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 33560.8984 - val_loss: 38467.5625\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 33385.9766 - val_loss: 38385.5273\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 33666.1953 - val_loss: 38300.0391\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 33536.1797 - val_loss: 38211.9805\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 32985.3359 - val_loss: 38120.3320\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 33221.7383 - val_loss: 38023.4219\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 33096.5391 - val_loss: 37922.1133\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 32644.0488 - val_loss: 37816.4883\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 32495.9863 - val_loss: 37705.1602\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 32005.2148 - val_loss: 37586.0039\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 31631.2676 - val_loss: 37459.5469\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 31688.8320 - val_loss: 37327.8164\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 32093.5547 - val_loss: 37190.5430\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 32463.4785 - val_loss: 37049.4453\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 30733.1133 - val_loss: 36901.9180\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 31460.2949 - val_loss: 36750.1445\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 30779.2949 - val_loss: 36591.8438\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 29918.9121 - val_loss: 36425.8750\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 30094.5586 - val_loss: 36253.7188\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 29950.9648 - val_loss: 36075.0547\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 29449.4531 - val_loss: 35888.0664\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 28613.9785 - val_loss: 35694.0352\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 29245.9746 - val_loss: 35491.8125\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 28033.4844 - val_loss: 35278.5039\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 27738.0938 - val_loss: 35056.4766\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 26863.3223 - val_loss: 34825.4648\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 27344.3770 - val_loss: 34584.9570\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 26643.8965 - val_loss: 34333.6523\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 25994.5918 - val_loss: 34072.7617\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 26205.4336 - val_loss: 33803.8867\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 24995.2617 - val_loss: 33524.8750\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 25579.5391 - val_loss: 33237.1406\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 24469.3184 - val_loss: 32937.8828\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 22688.6133 - val_loss: 32629.2676\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 23431.4141 - val_loss: 32309.7090\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 22974.6250 - val_loss: 31979.0645\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 24998.7480 - val_loss: 31641.9512\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 22095.7324 - val_loss: 31294.0039\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 20504.6328 - val_loss: 30936.1172\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 20517.3672 - val_loss: 30568.3750\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 18779.5449 - val_loss: 30186.4023\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 20200.0723 - val_loss: 29797.8281\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 18103.6504 - val_loss: 29397.8516\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 17025.4609 - val_loss: 28986.6777\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 17770.2422 - val_loss: 28565.4805\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 15090.5059 - val_loss: 28132.7773\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 17154.2188 - val_loss: 27694.8262\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 14884.5352 - val_loss: 27248.9141\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 12321.0566 - val_loss: 26796.1934\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 12297.4961 - val_loss: 26337.5195\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 14996.0693 - val_loss: 25874.0020\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 11446.9521 - val_loss: 25407.3848\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 12949.3594 - val_loss: 24936.1094\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 12727.8877 - val_loss: 24460.4883\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 11549.8145 - val_loss: 23981.1895\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 11093.3574 - val_loss: 23500.7871\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 8626.2275 - val_loss: 23022.2148\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8664.8301 - val_loss: 22544.1426\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 11846.6592 - val_loss: 22069.5566\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8368.1855 - val_loss: 21600.0371\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7949.6582 - val_loss: 21139.7695\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 6570.9736 - val_loss: 20687.2344\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 6644.0864 - val_loss: 20249.8770\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 5318.6025 - val_loss: 19830.0859\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3422.9717 - val_loss: 19438.2363\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 7413.5601 - val_loss: 19060.8887\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5320.3882 - val_loss: 18701.6113\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6026.7651 - val_loss: 18366.5820\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3510.6738 - val_loss: 18054.0059\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 5825.2803 - val_loss: 17754.6738\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4172.2163 - val_loss: 17490.5762\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3441.5457 - val_loss: 17234.9043\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4050.3535 - val_loss: 17005.0117\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2030.5244 - val_loss: 16794.5391\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3342.0884 - val_loss: 16598.9297\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3108.9224 - val_loss: 16429.6543\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3675.5225 - val_loss: 16276.5576\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1754.3944 - val_loss: 16140.1650\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4917.1533 - val_loss: 16022.0264\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2285.1804 - val_loss: 15930.1611\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3291.9089 - val_loss: 15865.8604\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3051.8928 - val_loss: 15826.2422\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3884.4026 - val_loss: 15797.5068\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2327.4260 - val_loss: 15764.4092\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2899.4702 - val_loss: 15739.0303\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2190.6084 - val_loss: 15705.4609\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4683.1353 - val_loss: 15699.7451\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5346.4160 - val_loss: 15684.8232\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1829.4901 - val_loss: 15663.3428\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2302.8372 - val_loss: 15649.7129\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2390.9661 - val_loss: 15625.0225\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 4035.9592 - val_loss: 15588.5000\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1329.2144 - val_loss: 15549.9922\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1900.7822 - val_loss: 15504.5234\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2115.5657 - val_loss: 15449.8770\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3416.4766 - val_loss: 15399.2988\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1766.5717 - val_loss: 15357.9268\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1362.5946 - val_loss: 15319.4102\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2499.0212 - val_loss: 15281.4043\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1666.1808 - val_loss: 15249.6289\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3231.0200 - val_loss: 15216.8838\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1188.2205 - val_loss: 15190.5449\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 745.3456 - val_loss: 15166.6943\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2019.4478 - val_loss: 15146.0010\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2443.4126 - val_loss: 15132.9111\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2284.6189 - val_loss: 15129.3252\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3184.4290 - val_loss: 15118.4492\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4698.0190 - val_loss: 15094.8701\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2525.7793 - val_loss: 15070.4307\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1672.6486 - val_loss: 15041.5732\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1416.7479 - val_loss: 15016.3213\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1765.6029 - val_loss: 14983.6289\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1534.6042 - val_loss: 14957.0303\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1923.0238 - val_loss: 14927.8076\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1793.8635 - val_loss: 14893.7041\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1887.2612 - val_loss: 14861.0117\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3267.2834 - val_loss: 14823.0254\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2015.6918 - val_loss: 14782.0059\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2425.2974 - val_loss: 14742.4824\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2367.7649 - val_loss: 14704.0469\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4331.4717 - val_loss: 14670.1152\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1723.3772 - val_loss: 14636.1904\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1407.1571 - val_loss: 14600.1309\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2770.0479 - val_loss: 14568.7744\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1811.7037 - val_loss: 14544.5908\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1735.2944 - val_loss: 14519.3330\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1931.3365 - val_loss: 14498.1221\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2203.4932 - val_loss: 14492.7881\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1575.4817 - val_loss: 14499.9248\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2303.6631 - val_loss: 14503.9082\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3153.7170 - val_loss: 14499.3760\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2922.2134 - val_loss: 14493.5049\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2940.3516 - val_loss: 14480.0771\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1455.3015 - val_loss: 14461.2559\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2797.3191 - val_loss: 14439.4766\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2685.7798 - val_loss: 14414.3525\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1219.4830 - val_loss: 14392.4365\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2382.9895 - val_loss: 14365.7324\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1828.1166 - val_loss: 14354.5332\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1800.3655 - val_loss: 14354.9824\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1329.3668 - val_loss: 14347.2178\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2145.0903 - val_loss: 14338.4297\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2124.8970 - val_loss: 14322.2520\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1634.6451 - val_loss: 14303.3672\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1698.0085 - val_loss: 14287.0195\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3203.0767 - val_loss: 14283.8633\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2185.1143 - val_loss: 14280.3477\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1337.1055 - val_loss: 14270.9756\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 729.8523 - val_loss: 14260.1152\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1940.5846 - val_loss: 14252.2207\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2413.6929 - val_loss: 14244.5654\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1509.8840 - val_loss: 14235.6787\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1512.2410 - val_loss: 14233.2764\n",
      "Mean Squared Error: 5507.31396484375\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "Prediction2: [[87.73058]]\n",
      "El salario es promedio\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Codificación one-hot para las variables categóricas\n",
    "df_encoded = pd.get_dummies(df, columns=['experience_level', 'employment_type', 'job_title', 'salary_currency', 'employee_residence', 'company_location', 'company_size'])\n",
    "\n",
    "# Dividir el dataset en datos de entrenamiento y de prueba\n",
    "X = df_encoded.drop('salary', axis=1).values\n",
    "y = df_encoded['salary'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Escalar los datos para mejorar la eficiencia del modelo\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Definir el modelo\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(8, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.001))\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=64, verbose=1, validation_split=0.2)\n",
    "\n",
    "# Evaluar el modelo\n",
    "mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Realizar una predicción\n",
    "prediction2 = model.predict(X_test[:1])\n",
    "print(\"Prediction2:\", prediction)\n",
    "\n",
    "# Clasificar el salario\n",
    "salary = y_test[:1][0]\n",
    "if salary > 80000:\n",
    "    print(\"El salario es alto\")\n",
    "elif salary > 50000:\n",
    "    print(\"El salario es medio\")\n",
    "else:\n",
    "    print(\"El salario es promedio\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Version 2 del Modelo 2__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingreso promedio en US: $193171.19\n",
      "Ingreso promedio en GB: $61879.28\n",
      "Ingreso promedio en DE: $70812.32\n",
      "Ingreso promedio en FR: $56884.73\n",
      "Ingreso promedio en CA: $114575.64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Agarramos las columnas que vamos a utilizar y codificamos las ubicaciones de las empresas\n",
    "df = df[['company_location', 'salary']]\n",
    "le = LabelEncoder()\n",
    "df['company_location'] = le.fit_transform(df['company_location'])\n",
    "\n",
    "# Separar los datos de entrada y salida\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "# Entrenar el modelo\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predecir los ingresos promedio por ubicación\n",
    "locations = le.transform(['US', 'GB', 'DE', 'FR', 'CA'])\n",
    "predicted_salaries = model.predict(locations.reshape(-1, 1))\n",
    "\n",
    "# Decodificar las ubicaciones de las empresas\n",
    "decoded_locations = le.inverse_transform(locations)\n",
    "\n",
    "# Imprimir los resultados\n",
    "for location, salary in zip(decoded_locations, predicted_salaries):\n",
    "    print(f\"Ingreso promedio en {location}: ${salary:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión del modelo es: 0.68\n"
     ]
    }
   ],
   "source": [
    "# Evaluar la precisión del modelo\n",
    "y_pred = model.predict(X)\n",
    "accuracy = r2_score(y, y_pred)\n",
    "print(f\"La precisión del modelo es: {accuracy:.2f}\")    #Por la complejidad y la naturaleza de los datos es dificil alcanzar el 0.7. Esta es la max precision posible"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Version 1 Modelo 3__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __INVESTIGACION DE LAS FILAS UNICAS EN CADA COLUMNA__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2020', '2021', '2022'], dtype=object)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['work_year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MI', 'SE', 'EN', 'EX'], dtype=object)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['experience_level'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['FT', 'CT', 'PT', 'FL'], dtype=object)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['employment_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Data Scientist', 'Machine Learning Scientist',\n",
       "       'Big Data Engineer', 'Product Data Analyst',\n",
       "       'Machine Learning Engineer', 'Data Analyst', 'Lead Data Scientist',\n",
       "       'Business Data Analyst', 'Lead Data Engineer', 'Lead Data Analyst',\n",
       "       'Data Engineer', 'Data Science Consultant', 'BI Data Analyst',\n",
       "       'Director of Data Science', 'Research Scientist',\n",
       "       'Machine Learning Manager', 'Data Engineering Manager',\n",
       "       'Machine Learning Infrastructure Engineer', 'ML Engineer',\n",
       "       'AI Scientist', 'Computer Vision Engineer',\n",
       "       'Principal Data Scientist', 'Data Science Manager', 'Head of Data',\n",
       "       '3D Computer Vision Researcher', 'Data Analytics Engineer',\n",
       "       'Applied Data Scientist', 'Marketing Data Analyst',\n",
       "       'Cloud Data Engineer', 'Financial Data Analyst',\n",
       "       'Computer Vision Software Engineer',\n",
       "       'Director of Data Engineering', 'Data Science Engineer',\n",
       "       'Principal Data Engineer', 'Machine Learning Developer',\n",
       "       'Applied Machine Learning Scientist', 'Data Analytics Manager',\n",
       "       'Head of Data Science', 'Data Specialist', 'Data Architect',\n",
       "       'Finance Data Analyst', 'Principal Data Analyst',\n",
       "       'Big Data Architect', 'Staff Data Scientist', 'Analytics Engineer',\n",
       "       'ETL Developer', 'Head of Machine Learning', 'NLP Engineer',\n",
       "       'Lead Machine Learning Engineer', 'Data Analytics Lead'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['job_title'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([233, 162, 252, 126,  75, 237, 120,  17,  52,  40, 200, 171, 178,\n",
       "       254, 245, 199,   1, 195, 214, 168, 201, 193, 267,  25, 176, 238,\n",
       "        86, 207, 218, 190, 225,   6, 159,   0,  58, 202, 198,  10, 256,\n",
       "        61, 117,   8, 222, 260, 170, 203,  46, 177,  30,  32,  59,  16,\n",
       "        47, 119, 191, 185, 213, 196, 121, 246, 179, 151, 186, 164, 228,\n",
       "        62, 229, 205, 103, 242,  70, 165, 145,  31, 247, 223, 240, 180,\n",
       "       149, 249, 182, 148, 257, 127, 255,  78, 234, 152, 235, 253, 144,\n",
       "       153, 211,  81,  90, 102, 265,  51,  67,  89, 150, 212,  96, 206,\n",
       "       104, 216, 172, 194, 100, 243, 143, 184, 217, 241,  79, 142, 110,\n",
       "        35,  72, 261, 109, 187, 208, 125, 174, 231, 133, 197, 115, 111,\n",
       "       160, 189, 192,  33, 161, 183,  91,  50, 209, 173,  12, 219, 135,\n",
       "       262,  41, 204, 140, 263,  20, 239,   4, 226,  21, 232,  14,  84,\n",
       "        36,   5, 251,  55, 271,  71,  38, 268,  28,  11,  43, 156, 169,\n",
       "        97,  34, 258, 112,  49, 146,  92,  39,  26,  23,  98,  99, 157,\n",
       "        54,  15,  44, 264, 147, 101, 105,  85,  13, 266,  24,  93,  57,\n",
       "       136, 122, 259, 221,  60,  53, 118,  95,  48, 131,  73,  56,   3,\n",
       "       220, 166,  94,  69,   9, 139,  87, 132,  83,  27, 248, 113, 224,\n",
       "        22, 155,  88, 250,   2,  65,  63, 154,  74,   7, 116, 175, 141,\n",
       "       134, 167, 210, 227, 215,  37,  80, 181, 188, 108, 244, 138, 124,\n",
       "       163, 137,  66, 130, 270,  18, 269, 129, 107, 128, 236, 114, 106,\n",
       "        64,  29, 230,  76, 158,  19, 123,  68,  77,  82,  42,  45])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['salary'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['EUR', 'USD', 'GBP', 'HUF', 'INR', 'JPY', 'CNY', 'MXN', 'CAD',\n",
       "       'DKK', 'PLN', 'SGD', 'CLP', 'BRL', 'TRY', 'AUD', 'CHF'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['salary_currency'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['79833', '59303', '15966', '148261', '62726', '49268', '79197',\n",
       "       '76833', '95746', '94564', '63831', '130026', '90734', '61467',\n",
       "       '24823', '88654', '173762', '141846', '65013', '87932', '54957',\n",
       "       '162674'], dtype=object)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['salary_in_usd'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DE', 'JP', 'GB', 'HN', 'US', 'HU', 'NZ', 'FR', 'IN', 'PK', 'PL',\n",
       "       'PT', 'CN', 'GR', 'AE', 'NL', 'MX', 'CA', 'AT', 'NG', 'PH', 'ES',\n",
       "       'DK', 'RU', 'IT', 'HR', 'BG', 'SG', 'BR', 'IQ', 'VN', 'BE', 'UA',\n",
       "       'MT', 'CL', 'RO', 'IR', 'CO', 'MD', 'KE', 'SI', 'HK', 'TR', 'RS',\n",
       "       'PR', 'LU', 'JE', 'CZ', 'AR', 'DZ', 'TN', 'MY', 'EE', 'AU', 'BO',\n",
       "       'IE', 'CH'], dtype=object)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['employee_residence'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', '50', '100'], dtype=object)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['remote_ratio'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DE', 'JP', 'GB', 'HN', 'US', 'HU', 'NZ', 'FR', 'IN', 'PK', 'CN',\n",
       "       'GR', 'AE', 'NL', 'MX', 'CA', 'AT', 'NG', 'ES', 'PT', 'DK', 'IT',\n",
       "       'HR', 'LU', 'PL', 'SG', 'RO', 'IQ', 'BR', 'BE', 'UA', 'IL', 'RU',\n",
       "       'MT', 'CL', 'IR', 'CO', 'MD', 'KE', 'SI', 'CH', 'VN', 'AS', 'TR',\n",
       "       'CZ', 'DZ', 'EE', 'MY', 'AU', 'IE'], dtype=object)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['company_location'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['L', 'S', 'M'], dtype=object)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['company_size'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
       "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
       "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
       "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
       "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
       "       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
       "       325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
       "       338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
       "       351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
       "       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
       "       377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
       "       390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n",
       "       403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415,\n",
       "       416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428,\n",
       "       429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441,\n",
       "       442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454,\n",
       "       455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
       "       468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480,\n",
       "       481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n",
       "       494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506,\n",
       "       507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519,\n",
       "       520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532,\n",
       "       533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545,\n",
       "       546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558,\n",
       "       559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571,\n",
       "       572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584,\n",
       "       585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597,\n",
       "       598, 599, 600, 601, 602, 603, 604, 605, 606], dtype=int64)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Column1'].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
